## kubernetes集群的通信
我们在[上一篇](./kubernetes_network_01.md)主要介绍了docker同宿主机，以及访问外部主机的的通信方式与原理，并且在最后也提出了容器跨主机访问的时候所需要用的网络插件Flannal和Calico，那么本篇我们就主要聊一下这个网络插件的工作原理，以及kubernetes集群的通信。

为什么要跨过介绍容器跨主机通信而直接到kubernetes集群的通信呢，这是因为kubernetes主要的功能就是容器的编排，虽然kubernetes的最小单元是pod，但是pod的核心依然是容器，所以，kubernetes集群pod之间的通信原理其实和容器通信的原理是一致的，所以我们直接介绍kubernetes集群的通信。

在上一篇中我们提到，docker的每个宿主机上都会起一个类似网桥功能的docker0，用于将容器中的请求“拿出”到主机中，然后再根据IP表进行下一步的转发，kubernetes同样也实现了这样的一个“网桥”就是cni0，如果说docker0网桥上挂的是一个一个容器，那么cni0网桥上挂的就是一个一个pod，而挂起的方式同样是通过Veth Pair设备实现的。那为什么kubernetes不用docker的docker0而要自己实现一个cni0呢？这就和pod的网络配置有关了。

```
    pod的网络配置：
    由于容器的本质是进程，但是一般情况下任务是由多个进程共同执行的，所以需要pod就将多个相关容器组成一个执行单元，而这个执行
单元需要共用同一套网络栈，这个时候cni0就派上用场了。细心的你会发现当你起一个pod的时候，在宿主机执行docker ps 的时候会发现
会先起一个pause-amd64容器，它就会先调用CNI插件，为这个pod配置网络资源，这个容器很小很小基本上是不占用资源的，所以它不会
影响pod中其他容器的运行，然后启动定义的一组容器，然后这一组容器共享这一套网络资源。
```

	
